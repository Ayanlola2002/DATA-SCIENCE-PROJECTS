{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Wrangling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/Wtm3zucU2biB78vKIiPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayanlola2002/DATA-SCIENCE-PROJECTS/blob/master/Data_Wrangling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vs4GuGkmMBT",
        "outputId": "07a2a0bb-df63-4d48-c689-edb5cb4cacff"
      },
      "source": [
        "#mounting Gdrive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pJYQGNJmYeQ"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from datetime import date, datetime\r\n",
        "import holidays"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ennCbmMYmdZj"
      },
      "source": [
        "df_5min=pd.read_csv('/content/drive/MyDrive/NEEDENERGY/data_five_min_id_47803.csv')\r\n",
        "df_hourly=pd.read_csv('/content/drive/MyDrive/NEEDENERGY/data_hourly_id_47803.csv')\r\n",
        "df_daily=pd.read_csv('/content/drive/MyDrive/NEEDENERGY/data_daily_id_47803.csv')\r\n",
        "df_monthly=pd.read_csv('/content/drive/MyDrive/NEEDENERGY/data_monthly_id_47803.csv')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W6rx8jU_wit"
      },
      "source": [
        "#numericalcol = [f for f in df_5min.columns if df_5min.dtypes[f] != 'object']\r\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAyetgiqM_E5"
      },
      "source": [
        "#df_5min[numericalcol]\r\n",
        "#miss=df_5min[numericalcol].columns[df_5min[numericalcol].isnull().any()].tolist()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QUUqzO5meJX"
      },
      "source": [
        "##Wrangling Helper Functions to Use\r\n",
        "\"\"\"\r\n",
        "This function do the reading of data file from file path(i.e drive)\r\n",
        "\"\"\"\r\n",
        "def do_read_data(datafile, dateCol):\r\n",
        "    ''' Read the excel file'''\r\n",
        "    df = pd.read_excel(datafile, index_col= dateCol, parse_dates=True)\r\n",
        "    return df\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "This function Shows Information of the Data file\r\n",
        "Like The Header columns, Dimension of Data\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "def do_data_information(datafile, info = False, head = True, shape = True):\r\n",
        "    ''' Look at the info of the data'''\r\n",
        "    \r\n",
        "    if not isinstance(datafile, pd.DataFrame):\r\n",
        "        df = read_data(datafile)\r\n",
        "    else:\r\n",
        "        df = datafile\r\n",
        "    \r\n",
        "    ## Information of data\r\n",
        "    if info:\r\n",
        "        df.info()\r\n",
        "        \r\n",
        "    print('--------------------------------------------------------------------')\r\n",
        "    print('First rows')\r\n",
        "    # Show first rows\r\n",
        "    if head:\r\n",
        "        print(df.head())\r\n",
        "        \r\n",
        "    print('--------------------------------------------------------------------')\r\n",
        "    print('Dimension of the data')\r\n",
        "    ## Dimension of data\r\n",
        "    if shape:\r\n",
        "        print(f'This data has {df.shape[0]} rows and {df.shape[1]} variables') \r\n",
        "        \r\n",
        "###This function returns the number of distinct values from any columns       \r\n",
        "def do_unique_values(df, col):\r\n",
        "    ''' Find the unique values of a columns'''\r\n",
        "    return df[col].nunique()\r\n",
        "\r\n",
        "\r\n",
        "###This function statistically describe the data values\r\n",
        "def do_description(df, col = None):\r\n",
        "    ''' Give the statistic summary'''\r\n",
        "    \r\n",
        "    ## Summary statistic\r\n",
        "    if col is not None:\r\n",
        "        stat = df[col].describe()\r\n",
        "    else:\r\n",
        "        stat = df.describe()\r\n",
        "\r\n",
        "    ## Change the name of the 50% index to median\r\n",
        "    idx = stat.index.tolist()\r\n",
        "    idx[5] = 'median'\r\n",
        "    stat.index = idx\r\n",
        "    return stat\r\n",
        "\r\n",
        "###This function finds % Missing values\r\n",
        "def check_missing_value(data):\r\n",
        "    ## percentage of missing values\r\n",
        "    n = data.isnull().sum().sort_values(ascending=False)/ len(data) * 100\r\n",
        "    return n\r\n",
        "\r\n",
        "def check_missing_values_table(df):\r\n",
        "  mis_val = df.isnull().sum()\r\n",
        "  mis_val_percent = 100 * df.isnull().sum() / len(df)\r\n",
        "  mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\r\n",
        "  mis_val_table_ren_columns = mis_val_table.rename(\r\n",
        "  columns = {0 : 'Missing Values', 1 : '% of Total Values'})\r\n",
        "  mis_val_table_ren_columns = mis_val_table_ren_columns[\r\n",
        "      mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\r\n",
        "  '% of Total Values', ascending=False).round(1)\r\n",
        "  print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \r\n",
        "      \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\r\n",
        "        \" columns that have missing values.\")\r\n",
        "  return mis_val_table_ren_columns\r\n",
        "\r\n",
        "\r\n",
        "###This function finds the amount of each values in a dataset\r\n",
        "def do_valueCount(data, col, perc = False):\r\n",
        "    ''' calculate the proportion in fraction or percentage'''\r\n",
        "    if not perc:\r\n",
        "      return data[col].value_counts()\r\n",
        "    else:\r\n",
        "      return data[col].value_counts(normalize = True).mul(100).round(1).astype(str) + '%'    \r\n",
        "### This function count number of unique values for all columns in a dataset\r\n",
        "def do_nunique_value(df):\r\n",
        "  print('The Numver of unique values per columns are as follows')\r\n",
        "  print(df.agg(['nunique']).T)\r\n",
        "\r\n",
        "\r\n",
        "###For date time conversion start here   \r\n",
        "def do_date_col_convertion(df, column_name):\r\n",
        "  df[column_name]=pd.to_datetime(df[column_name])\r\n",
        "  return df[column_name]\r\n",
        "\r\n",
        "def do_time_col_convertion(df, column_name):\r\n",
        "    return pd.to_timedelta(df[column_name])\r\n",
        "\r\n",
        "\r\n",
        "def do_datepart_extraction(data):\r\n",
        "  data['date'] = pd.to_datetime(data['date'])\r\n",
        "  data['year'] = data['date'].dt.year\r\n",
        "  data['quarter']=data['date'].dt.quarter\r\n",
        "  data['month'] = data['date'].dt.month\r\n",
        "  data['weekday'] = data['date'].dt.weekday\r\n",
        "  data['day_name'] = data['date'].dt.day_name()\r\n",
        "  data['dayofyear'] = data['date'].dt.dayofyear\r\n",
        "  data['day'] =data['date'].dt.day\r\n",
        "  data['hour'] = data['date'].dt.hour\r\n",
        "  data['minute'] =data['date'].dt.minute\r\n",
        "  data['Season']=data['Month'].apply(do_year_season)\r\n",
        "  data[Day_Season]=data['hour'].apply(do_day_season)\r\n",
        "  return data\r\n",
        "\r\n",
        "'''def do_datepart_extraction(data, col):\r\n",
        "    # Create year, month, month name and day from a datetime column\r\n",
        "    #data[col] = convert(data, col)\r\n",
        "    \r\n",
        "    data['Year'] = pd.DatetimeIndex(data[col]).year\r\n",
        "    data['Month'] = pd.DatetimeIndex(data[col]).month\r\n",
        "    data['Day'] = pd.DatetimeIndex(data[col]).day\r\n",
        "    data['weekday'] = pd.DatetimeIndex(data[col]).weekday\r\n",
        "    data['weekday_name'] = pd.DatetimeIndex(data[col]).day_name()\r\n",
        "    data['Hour'] = pd.DatetimeIndex(data[col]).hour\r\n",
        "    data['Quarter'].pd.DatetimeIndex(data[col]).Quarter\r\n",
        "\r\n",
        "    return data\r\n",
        "'''    \r\n",
        "\r\n",
        "def do_year_season(data):\r\n",
        "    ''' create season'''\r\n",
        "    if data['month'] in (1, 2, 12):\r\n",
        "        return \"Winter\"\r\n",
        "    elif data['month'] in (3, 4, 5):\r\n",
        "        return \"Spring\"\r\n",
        "    elif data['month'] in (6, 7, 8):\r\n",
        "        return \"Summer\"\r\n",
        "    elif data['month'] in (9, 10, 11):\r\n",
        "        return \"Fall\"\r\n",
        "    else:\r\n",
        "        return \"Other\"\r\n",
        "\r\n",
        "def do_day_season(data):\r\n",
        "  if data['hour'] >=0 and data['Hour'] < 5:\r\n",
        "    return \"Midnight\"\r\n",
        "  elif data['hour'] >=5 and data['Hour'] < 12:\r\n",
        "    return \"Morning\"\r\n",
        "  elif data['hour'] >=12 and data['Hour'] < 17:\r\n",
        "    return \"Afternoon\"\r\n",
        "  elif data['hour'] >=17 and data['Hour'] < 19:\r\n",
        "    return \"Evening\"\r\n",
        "  else:\r\n",
        "    return \"Night\"\r\n",
        "\r\n",
        "\r\n",
        "def get_holidays(startYear = 2018, endYear = 2025, countryCode = 'ZA'):\r\n",
        "  \"\"\"\r\n",
        "    Takes in a start and end date, and start and end year.\r\n",
        "    \r\n",
        "    Produces a dataframe with a daily date and columns:\r\n",
        "    holiday - 'Y' for holiday\r\n",
        "    holidayName - name of the holiday if holiday is 'Y'\r\n",
        "    \r\n",
        "    Returns a dataframe\r\n",
        "  \"\"\"\r\n",
        "  holidayDict = {}\r\n",
        "  for i in range(startYear, endYear):\r\n",
        "    for date, name in sorted(holidays.CountryHoliday(countryCode,years=[i]).items()):\r\n",
        "      holidayDict[date] = name\r\n",
        "      holiday_df = pd.DataFrame(list(holidayDict.items()),columns = ['day','holidayName'])\r\n",
        "      holiday_df['day'] = pd.to_datetime(holiday_df['day'], format='%Y-%M-%d').dt.date\r\n",
        "    return holiday_df\r\n",
        "    \r\n",
        "\r\n",
        "    \r\n",
        "def get_days(start = '1/1/2018',startYear = 2018, end = '31/12/2025',endYear = 2025, countryCode = 'ZA'):\r\n",
        "    \"\"\"\r\n",
        "    Takes in a start and end date, and start and end year.\r\n",
        "    \r\n",
        "    Produces a dataframe with a daily date and columns:\r\n",
        "    weekend - 'Y' for weendend and 'N' for workday\r\n",
        "    dayOfweek - numerical day of the week identifier 0 for monday\r\n",
        "    weekNum - numerical number of the week \r\n",
        "    holiday - 'Y' for holiday\r\n",
        "    holidayName - name of the holiday if holiday is 'Y'\r\n",
        "    \r\n",
        "    Returns a dataframe\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    #generate the range of daily dates\r\n",
        "    dates = pd.date_range(start = start, end = end)\r\n",
        "    date_df = pd.DataFrame(dates, columns = ['day'])\r\n",
        "    date_df['day'] = pd.to_datetime(date_df['day'], format='%Y-%M-%d')\r\n",
        "    country_holidays = get_holidays(startYear = startYear, endYear = endYear, countryCode = countryCode)\r\n",
        "    \r\n",
        "    date_df['dayName'] = pd.DatetimeIndex(date_df['day']).day_name()\r\n",
        "    date_df['dayOfWeek'] = date_df['day'].dt.dayofweek\r\n",
        "    date_df['weekend'] = date_df['dayOfWeek'].apply(lambda x: 'Y' if x>4 else 'N')\r\n",
        "    date_df['weekNum'] = date_df['day'].dt.week\r\n",
        "    date_df['holiday'] = date_df['day'].apply(lambda x: 'Y' if x in country_holidays['day'].values else 'N')\r\n",
        "    date_df['day'] = date_df['day'].dt.date\r\n",
        "    date_df = date_df.merge(country_holidays, on='day', how='left', indicator=False)\r\n",
        "    \r\n",
        "    date_df.to_csv(f'../data/public_holidays_weekends.csv', index=False)\r\n",
        "    return date_df\r\n",
        "\r\n",
        "\r\n",
        "def Impute_nan(df):\r\n",
        "  numericalcol = [f for f in df_5min.columns if df_5min.dtypes[f] != 'object']\r\n",
        "  missing_col=df_5min[numericalcol].columns[df_5min[numericalcol].isnull().any()].tolist()\r\n",
        "  #Technique 1: Using mean to impute the missing values\r\n",
        "  for i in missing_col:\r\n",
        "    df.loc[df.loc[:,i].isnull(),i]=df.loc[:,i].mean()\r\n",
        "\r\n",
        "def Impute_na(df):\r\n",
        "  numericalcol = [f for f in df_5min.columns if df_5min.dtypes[f] != 'object']\r\n",
        "  missing_col=df_5min[numericalcol].columns[df_5min[numericalcol].isnull().any()].tolist()\r\n",
        "  #Technique 1: Using mean to impute the missing values\r\n",
        "  for i in missing_col:\r\n",
        "     df[i].fillna(mean,inplace=True)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#df['holidays']=df['date'].apply(lambda(x: 'Hols' if x in get_holidays()['day'].values else 'Non-Hols'))\r\n",
        "\r\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vTU_osHm44W",
        "outputId": "c9a7c683-a5c3-4f45-9783-134632707fe4"
      },
      "source": [
        "#Data Description and Information for df_5min\r\n",
        "print(do_data_information(df_5min))\r\n",
        "print(do_data_information(df_hourly))\r\n",
        "print(do_data_information(df_daily))\r\n",
        "print(do_data_information(df_monthly))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------\n",
            "First rows\n",
            "                  date  ...                                   voltageHarmonics\n",
            "0  2020-03-15 18:15:00  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "1  2020-03-15 18:20:00  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "2  2020-03-15 18:25:00  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "3  2020-03-15 18:30:00  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "4  2020-03-15 18:35:00  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "\n",
            "[5 rows x 15 columns]\n",
            "--------------------------------------------------------------------\n",
            "Dimension of the data\n",
            "This data has 95649 rows and 15 variables\n",
            "None\n",
            "--------------------------------------------------------------------\n",
            "First rows\n",
            "                  date  ...                                   voltageHarmonics\n",
            "0  2020-03-15 18:00:00  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "1  2020-03-15 19:00:00  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "2  2020-03-15 20:00:00  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "3  2020-03-15 21:00:00  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "4  2020-03-15 22:00:00  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "\n",
            "[5 rows x 15 columns]\n",
            "--------------------------------------------------------------------\n",
            "Dimension of the data\n",
            "This data has 7979 rows and 15 variables\n",
            "None\n",
            "--------------------------------------------------------------------\n",
            "First rows\n",
            "         date  ...                                   voltageHarmonics\n",
            "0  2020-03-15  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "1  2020-03-16  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "2  2020-03-17  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "3  2020-03-18  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "4  2020-03-19  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "\n",
            "[5 rows x 15 columns]\n",
            "--------------------------------------------------------------------\n",
            "Dimension of the data\n",
            "This data has 335 rows and 15 variables\n",
            "None\n",
            "--------------------------------------------------------------------\n",
            "First rows\n",
            "         date  ...                                   voltageHarmonics\n",
            "0  2020-03-01  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "1  2020-04-01  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "2  2020-05-01  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "3  2020-06-01  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "4  2020-07-01  ...  [[], [], [], [], [], [], [], [], [], [], [], [...\n",
            "\n",
            "[5 rows x 15 columns]\n",
            "--------------------------------------------------------------------\n",
            "Dimension of the data\n",
            "This data has 12 rows and 15 variables\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdG15O0DoIvf"
      },
      "source": [
        "Data Quality Information:\r\n",
        "df_5min : The data has 95649 rows and 15 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpwtZCr1oBhO",
        "outputId": "dbda60f5-b250-46be-8fc7-f55b0904af17"
      },
      "source": [
        "#Checking Missing Values by percentage\r\n",
        "print(check_missing_values_table(df_5min))\r\n",
        "print(check_missing_values_table(df_daily))\r\n",
        "print(check_missing_values_table(df_hourly))\r\n",
        "print(check_missing_values_table(df_monthly))\r\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your selected dataframe has 15 columns.\n",
            "There are 0 columns that have missing values.\n",
            "Empty DataFrame\n",
            "Columns: [Missing Values, % of Total Values]\n",
            "Index: []\n",
            "Your selected dataframe has 15 columns.\n",
            "There are 0 columns that have missing values.\n",
            "Empty DataFrame\n",
            "Columns: [Missing Values, % of Total Values]\n",
            "Index: []\n",
            "Your selected dataframe has 15 columns.\n",
            "There are 0 columns that have missing values.\n",
            "Empty DataFrame\n",
            "Columns: [Missing Values, % of Total Values]\n",
            "Index: []\n",
            "Your selected dataframe has 15 columns.\n",
            "There are 0 columns that have missing values.\n",
            "Empty DataFrame\n",
            "Columns: [Missing Values, % of Total Values]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUDUqoBGTCcN",
        "outputId": "6a02a6a3-25a2-437d-c049-57f13e29e7c9"
      },
      "source": [
        "#Checking number of Unique values\r\n",
        "print(do_nunique_value(df_5min))\r\n",
        "print(do_nunique_value(df_hourly))\r\n",
        "print(do_nunique_value(df_daily))\r\n",
        "print(do_nunique_value(df_monthly))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Numver of unique values per columns are as follows\n",
            "                  nunique\n",
            "date                95649\n",
            "timestamp           95649\n",
            "consumption         76377\n",
            "solar               50268\n",
            "alwaysOn              955\n",
            "gridImport          66188\n",
            "gridExport              1\n",
            "selfConsumption         2\n",
            "selfSufficiency      5594\n",
            "active              95512\n",
            "reactive            95595\n",
            "voltages              389\n",
            "phaseVoltages         389\n",
            "currentHarmonics        1\n",
            "voltageHarmonics        1\n",
            "None\n",
            "The Numver of unique values per columns are as follows\n",
            "                  nunique\n",
            "date                 7979\n",
            "timestamp            7979\n",
            "consumption          7963\n",
            "solar                7876\n",
            "alwaysOn              714\n",
            "gridImport           7946\n",
            "gridExport              1\n",
            "selfConsumption         2\n",
            "selfSufficiency      3092\n",
            "active               7979\n",
            "reactive             7979\n",
            "voltages              280\n",
            "phaseVoltages         280\n",
            "currentHarmonics        1\n",
            "voltageHarmonics        1\n",
            "None\n",
            "The Numver of unique values per columns are as follows\n",
            "                  nunique\n",
            "date                  335\n",
            "timestamp             335\n",
            "consumption           335\n",
            "solar                 333\n",
            "alwaysOn              333\n",
            "gridImport            335\n",
            "gridExport              1\n",
            "selfConsumption         2\n",
            "selfSufficiency       310\n",
            "active                335\n",
            "reactive              335\n",
            "voltages               95\n",
            "phaseVoltages          95\n",
            "currentHarmonics        1\n",
            "voltageHarmonics        1\n",
            "None\n",
            "The Numver of unique values per columns are as follows\n",
            "                  nunique\n",
            "date                   12\n",
            "timestamp              12\n",
            "consumption            12\n",
            "solar                  12\n",
            "alwaysOn               12\n",
            "gridImport             12\n",
            "gridExport              1\n",
            "selfConsumption         1\n",
            "selfSufficiency        12\n",
            "active                 12\n",
            "reactive               12\n",
            "voltages               10\n",
            "phaseVoltages          10\n",
            "currentHarmonics        1\n",
            "voltageHarmonics        1\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOEm9i04noWh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "66e063fe-a319-495b-a7bd-f9ddb412dd8f"
      },
      "source": [
        "#Feature Engineering\r\n",
        "df_5min=do_datepart_extraction(df_5min)\r\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Month'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-db9dd584265a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Feature Engineering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_5min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_datepart_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_5min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-71-048fefdcfdd9>\u001b[0m in \u001b[0;36mdo_datepart_extraction\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'minute'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Season'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_year_season\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDay_Season\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_day_season\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Month'"
          ]
        }
      ]
    }
  ]
}